{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Fake News Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "VIZNcqpviyKN",
        "_LcEjMFZjKW3",
        "PVv0V-NTjndq",
        "XS9bdE4xlL6n",
        "CfpN7NCnktEZ",
        "RJZzBboFlXLk",
        "Ssj56Wvdl-uZ",
        "myxOmE1dpZKO",
        "QMVdEYS8sB11",
        "ApdcIrYqsM1j",
        "lSWly3G6viLv",
        "PDgxZQjhwlNf",
        "XXpgevJ7ws1O",
        "FOUBhWFVxEDh",
        "-SR9P4LhxHrz",
        "wEASOBefxs4j",
        "Nx1SL6yXx996",
        "PRRiJcB5y2Be",
        "IOxsqehkNYBg",
        "KSIzEmptNrn0",
        "ENXr0CucWD-B",
        "ua5vU7UbWNEo",
        "mISQC4HWWWUG",
        "i7H-1hhVWawx",
        "EPermr4jWdO_",
        "pYyJIde7OGYR",
        "4aMK5KJlZ8Yw",
        "AxveNnJJa0Pc",
        "LTz8ovhSa3oS",
        "llwE_T4EbBfF",
        "-plRjPVPbEN1",
        "4SGwBQ-gbGMA",
        "aZ97cqlAbanB",
        "0JuJGuWpb7Sx",
        "7Rzwq6omjaHX",
        "x0AAD9yznLW_"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIZNcqpviyKN"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bHmNkPnjEXc"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "import pickle\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from empath import Empath\n",
        "from nltk import tokenize\n",
        "import scipy.sparse as sp\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LcEjMFZjKW3"
      },
      "source": [
        "# Downloads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAFUTWT1jOlg"
      },
      "source": [
        "!pip install empath\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFnGllm8aefR"
      },
      "source": [
        "# Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym_UNsy8ahoG"
      },
      "source": [
        "# To see more of the output\n",
        "pd.set_option('display.max_colwidth', 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvqqmObdas3j"
      },
      "source": [
        "# Utility Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMVdEYS8sB11"
      },
      "source": [
        "## Cleaning Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80dd45jAr9xd"
      },
      "source": [
        "def clean_text(text):\n",
        "  text = re.sub('['+string.punctuation+']','', text)\n",
        "  text = re.sub(r\"[-()\\\"#/@â€™;:<>{}`+=~|.!?,]\", '', text)\n",
        "  text = text.lower().split()\n",
        "\n",
        "  stops = set(stopwords.words(\"english\"))\n",
        "  text = [w for w in text if w not in stops]\n",
        "  text = \" \".join(text)\n",
        "  \n",
        "  text = re.sub(r'[^a-zA-Z\\s]', u'', text, flags=re.UNICODE)\n",
        "  \n",
        "  text = text.split()\n",
        "  l = WordNetLemmatizer()\n",
        "  lemmatized_words = [l.lemmatize(word) for word in text if len(word) > 2]\n",
        "  text = \" \".join(lemmatized_words)\n",
        "    \n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRRiJcB5y2Be"
      },
      "source": [
        "## Confusion Matrix Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqwY8IXHy6AB"
      },
      "source": [
        "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=0)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('Actual label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePnhFYZSjZkJ"
      },
      "source": [
        "# Dataset Importing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVv0V-NTjndq"
      },
      "source": [
        "## Importing raw data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH2bLS0_jq5d"
      },
      "source": [
        "data = pd.read_csv(\"data.tsv\", delimiter='\\t', encoding='mac_roman')\n",
        "\n",
        "print(data.shape)\n",
        "print(data.columns)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhuCJIpNN-m2"
      },
      "source": [
        "data2 = pd.read_csv(\"data2.tsv\", delimiter='\\t', encoding='mac_roman')\n",
        "\n",
        "indexNames = data2[data2['our rating'] == 'false'].index\n",
        "data2.drop(indexNames , inplace=True)\n",
        "\n",
        "print(data2.shape)\n",
        "print(data2.columns)\n",
        "data2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfQyMngCOFa-"
      },
      "source": [
        "data = data.append(data2, ignore_index = True)\n",
        "\n",
        "print(data.shape)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS9bdE4xlL6n"
      },
      "source": [
        "## Data Manipulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfpN7NCnktEZ"
      },
      "source": [
        "### Label Capitalizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4EZ8_3Uk2x0"
      },
      "source": [
        "data.loc[data['our rating'] == 'true', 'our rating'] = 'TRUE'\n",
        "data.loc[data['our rating'] == 'false', 'our rating'] = 'FALSE'\n",
        "data.loc[data['our rating'] == 'partially false', 'our rating'] = 'PARTIALLY FALSE'\n",
        "data.loc[data['our rating'] == 'other', 'our rating'] = 'OTHER'\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJZzBboFlXLk"
      },
      "source": [
        "### Combining the **title** and **text** columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdT0njeklouJ"
      },
      "source": [
        "data['text'] = data['title'] + \" \" + data['text']\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssj56Wvdl-uZ"
      },
      "source": [
        "### Extra columns removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YxzM0PGmFRF"
      },
      "source": [
        "data.drop(columns=['title'], inplace = True)\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myxOmE1dpZKO"
      },
      "source": [
        "### Label Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7i24oM4pfqI"
      },
      "source": [
        "data['our rating'].value_counts()\n",
        "data['our rating'].value_counts().plot(kind = 'bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47cxoMOcr6L-"
      },
      "source": [
        "# First Approach - Simple TFIDF Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsmaTeSYwcO7"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApdcIrYqsM1j"
      },
      "source": [
        "### Creating an Additional Column with Cleaned Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waIVcheFsWCb"
      },
      "source": [
        "data['clean_text'] = data['text'].astype('str').apply(lambda x: clean_text(x))\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSWly3G6viLv"
      },
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDgxZQjhwlNf"
      },
      "source": [
        "### Word Cloud View: FALSE Label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBB7YUZ8vlpB"
      },
      "source": [
        "fake_data = data[data[\"our rating\"] == \"FALSE\"].astype('str')\n",
        "all_words = ' '.join([text for text in fake_data.text])\n",
        "\n",
        "word_cloud = WordCloud(width = 1000, height = 1000, max_font_size = 110, collocations = False).generate(all_words)\n",
        "\n",
        "plt.figure(figsize = (10, 7))\n",
        "plt.imshow(word_cloud, interpolation = 'bilinear')\n",
        "plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXpgevJ7ws1O"
      },
      "source": [
        "### Word Cloud View: TRUE Label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC6s-bUZwvF0"
      },
      "source": [
        "true_data = data[data[\"our rating\"] == \"TRUE\"].astype('str')\n",
        "all_words = ' '.join([text for text in true_data.text])\n",
        "\n",
        "word_cloud = WordCloud(width = 1000, height = 1000, max_font_size = 110, collocations = False).generate(all_words)\n",
        "plt.figure(figsize = (10, 7))\n",
        "plt.imshow(word_cloud, interpolation = 'bilinear')\n",
        "plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOUBhWFVxEDh"
      },
      "source": [
        "### Word Cloud View: PARTIALLY FALSE Label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9BwML55xA_Q"
      },
      "source": [
        "partially_false_data = data[data[\"our rating\"] == \"PARTIALLY FALSE\"].astype('str')\n",
        "all_words = ' '.join([text for text in partially_false_data.text])\n",
        "\n",
        "word_cloud = WordCloud(width = 1000, height = 1000, max_font_size = 110, collocations = False).generate(all_words)\n",
        "plt.figure(figsize = (10, 7))\n",
        "plt.imshow(word_cloud, interpolation = 'bilinear')\n",
        "plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SR9P4LhxHrz"
      },
      "source": [
        "### Word Cloud View: OTHER Label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_sO2fI-xDGo"
      },
      "source": [
        "other_data = data[data[\"our rating\"] == \"OTHER\"].astype('str')\n",
        "all_words = ' '.join([text for text in other_data.text])\n",
        "\n",
        "word_cloud = WordCloud(width = 1000, height = 1000, max_font_size = 110, collocations = False).generate(all_words)\n",
        "plt.figure(figsize = (10, 7))\n",
        "plt.imshow(word_cloud, interpolation = 'bilinear')\n",
        "plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEASOBefxs4j"
      },
      "source": [
        "## Train and Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvQp91a3xxS7"
      },
      "source": [
        "y = data['our rating'].astype('str') \n",
        "X_train, X_test, y_train, y_test = train_test_split(data['clean_text'], y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "print(X_train.head())\n",
        "print()\n",
        "print(y_train.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx1SL6yXx996"
      },
      "source": [
        "## TFIDF Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jsi0EGGNx_pU"
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(stop_words = 'english', ngram_range = (2, 2))\n",
        "\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "print(tfidf_vectorizer.get_feature_names()[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXiIQJjvzGrN"
      },
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMg7uW_e3Duc"
      },
      "source": [
        "### Naive-Bayes Alpha-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT8_i9a-3JVK"
      },
      "source": [
        "alphas = np.arange(0, 1, 0.1)\n",
        "\n",
        "def train_and_predict(alpha, x_train, x_test):\n",
        "    nb_classifier = MultinomialNB(alpha=alpha)\n",
        "    nb_classifier.fit(x_train, y_train)\n",
        "    pred = nb_classifier.predict(x_test)\n",
        "    score = metrics.accuracy_score(y_test, pred)\n",
        "    return score\n",
        "\n",
        "for alpha in alphas:\n",
        "  print('Alpha: ', alpha)\n",
        "  print('Score: ', train_and_predict(alpha, tfidf_train, tfidf_test))\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKUsqaNwzKVY"
      },
      "source": [
        "### Naive-Bayes (with best parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDBciskAzMtc"
      },
      "source": [
        "nb_classifier = MultinomialNB(alpha=0.0)\n",
        "nb_classifier.fit(tfidf_train, y_train)\n",
        "\n",
        "pred = nb_classifier.predict(tfidf_test)\n",
        "print(classification_report(y_test, pred))\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['FALSE', 'TRUE', 'PARTIALLY FALSE', 'OTHER'])\n",
        "print('Confusion Matrix: ')\n",
        "print(cm)\n",
        "plot_confusion_matrix(cm, classes=['FALSE', 'TRUE', 'PARTIALLY', 'OTHER'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FbF90jDDJBK"
      },
      "source": [
        "### K Nearest Neighbors Hyper-Parameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3kWj0NEDYbk"
      },
      "source": [
        "knn_params = RandomizedSearchCV(estimator=KNeighborsClassifier(), param_distributions={\n",
        "    \"leaf_size\": list(range(1, 50)),\n",
        "    \"n_neighbors\": list(range(1, 30)),\n",
        "    \"p\": [1, 2]\n",
        "}, n_iter=100, n_jobs=-1)\n",
        "\n",
        "result = knn_params.fit(tfidf_train, y_train)\n",
        "result_df = pd.DataFrame(result.cv_results_).loc[[result.best_index_]]\n",
        "\n",
        "print(result_df[\"params\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH-mY3FVzxZ9"
      },
      "source": [
        "### K Nearest Neighbors (with best parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pnErnzY140Q"
      },
      "source": [
        "knn_classifier = KNeighborsClassifier(p=2, n_neighbors=29, leaf_size=17)\n",
        "knn_classifier.fit(tfidf_train, y_train)\n",
        "\n",
        "pred = knn_classifier.predict(tfidf_test)\n",
        "print(classification_report(y_test, pred))\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['FALSE', 'TRUE', 'PARTIALLY FALSE', 'OTHER'])\n",
        "print('Confusion Matrix: ')\n",
        "print(cm)\n",
        "plot_confusion_matrix(cm, classes=['FALSE', 'TRUE', 'PARTIALLY', 'OTHER'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhI8CKHYEMTk"
      },
      "source": [
        "### Random Forest Hyper-Parameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4flnSNGOETFR"
      },
      "source": [
        "rf_params = RandomizedSearchCV(estimator=RandomForestClassifier(), param_distributions={\n",
        "    \"n_estimators\" : [200, 400, 600, 800, 1000],\n",
        "    \"max_features\" : ['auto', 'sqrt'],\n",
        "    \"max_depth\" : [10, 20, 30, 40, 50, None],\n",
        "    \"min_samples_split\" : [2, 5, 10],\n",
        "    \"min_samples_leaf\" : [1, 2, 4]\n",
        "}, n_iter=10, n_jobs=-1)\n",
        "\n",
        "result = rf_params.fit(tfidf_train, y_train)\n",
        "result_df = pd.DataFrame(result.cv_results_).loc[[result.best_index_]]\n",
        "\n",
        "print(result_df[\"params\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrDkO7nk2W0d"
      },
      "source": [
        "\n",
        "### Random Forest (with best parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErQJfp2M2Y4Q"
      },
      "source": [
        "rf_classifier = RandomForestClassifier(verbose=True, n_estimators = 1000, max_features = 'sqrt', max_depth = 50, min_samples_split = 2, min_samples_leaf = 2)\n",
        "rf_classifier.fit(tfidf_train, y_train)\n",
        "\n",
        "pred = rf_classifier.predict(tfidf_test)\n",
        "print(classification_report(y_test, pred))\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['FALSE', 'TRUE', 'PARTIALLY FALSE', 'OTHER'])\n",
        "print('Confusion Matrix: ')\n",
        "print(cm)\n",
        "plot_confusion_matrix(cm, classes=['FALSE', 'TRUE', 'PARTIALLY', 'OTHER'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2o7kWTLe2mNN"
      },
      "source": [
        "### Gradient Boosting (with default parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXyg2lsD2neJ"
      },
      "source": [
        "gb_classifier = GradientBoostingClassifier(verbose=True, n_estimators = 200)\n",
        "gb_classifier.fit(tfidf_train, y_train)\n",
        "\n",
        "pred = gb_classifier.predict(tfidf_test)\n",
        "print(classification_report(y_test, pred))\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['FALSE', 'TRUE', 'PARTIALLY FALSE', 'OTHER'])\n",
        "print('Confusion Matrix: ')\n",
        "print(cm)\n",
        "plot_confusion_matrix(cm, classes=['FALSE', 'TRUE', 'PARTIALLY', 'OTHER'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLaqKtQzNUCp"
      },
      "source": [
        "# Second Approach - POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOxsqehkNYBg"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh93C6qONa73"
      },
      "source": [
        "nlp = spacy.load('en')\n",
        "pos_tags_column = []\n",
        "\n",
        "for text in data['text'].astype('str'):\n",
        "    pos_tags = []\n",
        "    doc = nlp(text)\n",
        "    for token in doc:\n",
        "        pos_tags.append(token.pos_)\n",
        "    all_pos_tags = ' '.join(pos_tags)\n",
        "    pos_tags_column.append(all_pos_tags)\n",
        "    \n",
        "data['POS_text'] = pos_tags_column\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSIzEmptNrn0"
      },
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENXr0CucWD-B"
      },
      "source": [
        "### Counter Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffHNCsMrWGjM"
      },
      "source": [
        "token_space = tokenize.WhitespaceTokenizer()\n",
        "\n",
        "def counter(text, column_text, quantity):\n",
        "    words = ' '.join([text for text in text[column_text]])\n",
        "    token_phrase = token_space.tokenize(words)\n",
        "    frequency = nltk.FreqDist(token_phrase)\n",
        "    df_frequency = pd.DataFrame({\"Word\": list(frequency.keys()), \"Frequency\": list(frequency.values())})\n",
        "    df_frequency = df_frequency.nlargest(columns=\"Frequency\", n=quantity)\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    ax = sns.barplot(data=df_frequency, x=\"Word\", y=\"Frequency\", color='blue')\n",
        "    ax.set(ylabel=\"Count\")\n",
        "    plt.xticks(rotation='vertical')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua5vU7UbWNEo"
      },
      "source": [
        "### Most Frequent POS in FALSE Labeled texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgJgkCYJNtCJ"
      },
      "source": [
        "counter(data[data[\"our rating\"] == \"FALSE\"], \"POS_text\", 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mISQC4HWWWUG"
      },
      "source": [
        "### Most Frequent POS in TRUE Labeled texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI6XjXMMWaCp"
      },
      "source": [
        "counter(data[data[\"our rating\"] == \"TRUE\"], \"POS_text\", 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7H-1hhVWawx"
      },
      "source": [
        "### Most Frequent POS in PARTIALLY FALSE Labeled texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mk2BP-nMWc8W"
      },
      "source": [
        "counter(data[data[\"our rating\"] == \"PARTIALLY FALSE\"], \"POS_text\", 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPermr4jWdO_"
      },
      "source": [
        "### Most Frequent POS in OTHER Labeled texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnwHFKJfWeuv"
      },
      "source": [
        "counter(data[data[\"our rating\"] == \"OTHER\"], \"POS_text\", 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYyJIde7OGYR"
      },
      "source": [
        "## Train and Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSAx_YxCOKr6"
      },
      "source": [
        "y = data['our rating'].astype('str')\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['POS_text'], y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "print(X_train.head())\n",
        "print(y_train.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isnnntpWOUzk"
      },
      "source": [
        "## TFIDF Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IvlpcgPOZH3"
      },
      "source": [
        "pos_tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range = (2,2))\n",
        "pos_tfidf_train = pos_tfidf_vectorizer.fit_transform(X_train.astype('str'))\n",
        "pos_tfidf_test= pos_tfidf_vectorizer.transform(X_test.astype('str'))\n",
        "pos_tfidf_vectorizer.get_feature_names()[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5iXlbHCPKrc"
      },
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DCZGpALPOAC"
      },
      "source": [
        "### Naive-Bayes Alpha-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNcW6UTBPSlG"
      },
      "source": [
        "for alpha in alphas:\n",
        "  print('Alpha: ', alpha)\n",
        "  print('Score: ', train_and_predict(alpha, pos_tfidf_train, pos_tfidf_test))\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvnOniXSP2Ue"
      },
      "source": [
        "### Naive-Bayes (with best parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNbvcua8P9UL"
      },
      "source": [
        "nb_classifier = MultinomialNB(alpha = 0.0)\n",
        "nb_classifier.fit(pos_tfidf_train, y_train)\n",
        "\n",
        "pred = nb_classifier.predict(pos_tfidf_test)\n",
        "print(classification_report(y_test, pred))\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['FALSE', 'TRUE', 'PARTIALLY FALSE', 'OTHER'])\n",
        "print('Confusion Matrix: ')\n",
        "print(cm)\n",
        "plot_confusion_matrix(cm, classes=['FALSE', 'TRUE', 'PARTIALLY', 'OTHER'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtdC0tjXQQme"
      },
      "source": [
        "### K Nearest Neighbors Hyper-Parameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sze1XJqQUsM"
      },
      "source": [
        "knn_params = RandomizedSearchCV(estimator=KNeighborsClassifier(), param_distributions={\n",
        "    \"leaf_size\": list(range(1, 50)),\n",
        "    \"n_neighbors\": list(range(1, 30)),\n",
        "    \"p\": [1, 2]\n",
        "}, n_iter=100, n_jobs=-1)\n",
        "\n",
        "result = knn_params.fit(pos_tfidf_train, y_train)\n",
        "result_df = pd.DataFrame(result.cv_results_).loc[[result.best_index_]]\n",
        "\n",
        "print(result_df[\"params\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g58esPyYQlV0"
      },
      "source": [
        "### K Nearest Neighbors (with best parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vboSqk8SQklM"
      },
      "source": [
        "knn_classifier = KNeighborsClassifier(p = 1, n_neighbors = 25, leaf_size = 35)\n",
        "knn_classifier.fit(pos_tfidf_train, y_train)\n",
        "\n",
        "pred = knn_classifier.predict(pos_tfidf_test)\n",
        "print(classification_report(y_test, pred))\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['FALSE', 'TRUE', 'PARTIALLY FALSE', 'OTHER'])\n",
        "plot_confusion_matrix(cm, classes=['FALSE', 'TRUE', 'PARTIALLY', 'OTHER'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f190pgbSRC9h"
      },
      "source": [
        "### Random Forest Hyper-Parameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0D9LsRWfRGuu"
      },
      "source": [
        "rf_params = RandomizedSearchCV(estimator=RandomForestClassifier(), param_distributions={\n",
        "    \"n_estimators\" : [200, 400, 600, 800, 1000],\n",
        "    \"max_features\" : ['auto', 'sqrt'],\n",
        "    \"max_depth\" : [10, 20, 30, 40, 50, None],\n",
        "    \"min_samples_split\" : [2, 5, 10],\n",
        "    \"min_samples_leaf\" : [1, 2, 4]\n",
        "}, n_iter=10, n_jobs=-1)\n",
        "\n",
        "result = rf_params.fit(pos_tfidf_train, y_train)\n",
        "result_df = pd.DataFrame(result.cv_results_).loc[[result.best_index_]]\n",
        "\n",
        "print(result_df[\"params\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX--4qEfXapd"
      },
      "source": [
        "### Random Forest (with best parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPYLVV9WXdRZ"
      },
      "source": [
        "rf_classifier = RandomForestClassifier(n_estimators = 400, min_samples_split = 10, min_samples_leaf = 4, max_features = 'sqrt', max_depth = 30)\n",
        "rf_classifier.fit(pos_tfidf_train, y_train)\n",
        "\n",
        "pred = rf_classifier.predict(pos_tfidf_test)\n",
        "print(classification_report(y_test, pred))\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['FALSE', 'TRUE', 'PARTIALLY FALSE', 'OTHER'])\n",
        "print('Confusion Matrix: ')\n",
        "print(cm)\n",
        "plot_confusion_matrix(cm, classes=['FALSE', 'TRUE', 'PARTIALLY', 'OTHER'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opoAHt7gY_OT"
      },
      "source": [
        "### Gradient Boosting (with default parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUgzFS64ZDt3"
      },
      "source": [
        "gb_classifier = GradientBoostingClassifier(verbose=True, n_estimators = 200)\n",
        "gb_classifier.fit(pos_tfidf_train, y_train)\n",
        "\n",
        "pred = gb_classifier.predict(pos_tfidf_test)\n",
        "print(classification_report(y_test, pred))\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['FALSE', 'TRUE', 'PARTIALLY FALSE', 'OTHER'])\n",
        "print('Confusion Matrix: ')\n",
        "print(cm)\n",
        "plot_confusion_matrix(cm, classes=['FALSE', 'TRUE', 'PARTIALLY', 'OTHER'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7PQVVtmZ4gk"
      },
      "source": [
        "# Third Approach - Semantic Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aMK5KJlZ8Yw"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLZRTR-aZ_zo"
      },
      "source": [
        "lexicon = Empath()\n",
        "semantic = []\n",
        "count = 0\n",
        "\n",
        "for article in data['text'].astype('str'):\n",
        "    print(article)\n",
        "    d = lexicon.analyze(article, normalize=False)\n",
        "    print(d)\n",
        "    x = []\n",
        "    for key, value in d.items():\n",
        "        x.append(value)\n",
        "    x = np.asarray(x)\n",
        "    semantic.append(x)\n",
        "data['semantic_text'] = semantic\n",
        "\n",
        "print(data['semantic_text'].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LktMNbtIaKAP"
      },
      "source": [
        "categories = []\n",
        "a = lexicon.analyze(\"\")\n",
        "for key, value in a.items():\n",
        "    categories.append(key)\n",
        "    \n",
        "categories"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40U76SjOaOMG"
      },
      "source": [
        "sem = []\n",
        "for i in range(data.shape[0]):\n",
        "    a = []\n",
        "    for j in range(len(semantic[0])):\n",
        "        for k in range(int(semantic[i][j])):\n",
        "            a.append(categories[j])\n",
        "    b = \" \".join(a)\n",
        "    sem.append(b)\n",
        "data['semantics_text'] = sem\n",
        "\n",
        "print(data['semantics_text'].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxveNnJJa0Pc"
      },
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTz8ovhSa3oS"
      },
      "source": [
        "### Most Frequent Subjects in FALSE Labeled texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnlKduW3avtz"
      },
      "source": [
        "counter(data[data[\"our rating\"] == \"FALSE\"], \"semantics_text\", 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llwE_T4EbBfF"
      },
      "source": [
        "### Most Frequent Subjects in TRUE Labeled texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XH9KNz2bD_c"
      },
      "source": [
        "counter(data[data[\"our rating\"] == \"TRUE\"], \"semantics_text\", 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-plRjPVPbEN1"
      },
      "source": [
        "### Most Frequent Subjects in PARTIALLY FALSE Labeled texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-zy2m_sbF2J"
      },
      "source": [
        "counter(data[data[\"our rating\"] == \"PARTIALLY FALSE\"], \"semantics_text\", 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SGwBQ-gbGMA"
      },
      "source": [
        "### Most Frequent Subjects in OTHER Labeled texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVv9eqTabHXv"
      },
      "source": [
        "counter(data[data[\"our rating\"] == \"OTHER\"], \"semantics_text\", 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ97cqlAbanB"
      },
      "source": [
        "## Train and Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn0XinJtbdU-"
      },
      "source": [
        "y = data['our rating'].astype('str')\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['semantics_text'], y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "print(X_train.head())\n",
        "print(y_train.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JuJGuWpb7Sx"
      },
      "source": [
        "## TFIDF Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZEsPdrVb9VU"
      },
      "source": [
        "sem_tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,1))\n",
        "sem_tfidf_train = sem_tfidf_vectorizer.fit_transform(X_train.astype('str'))\n",
        "sem_tfidf_test = sem_tfidf_vectorizer.transform(X_test.astype('str'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fnpojXvcIF7"
      },
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52S5dWc-cSoW"
      },
      "source": [
        "### Naive-Bayes Alpha-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydVyy4CFcT3Z"
      },
      "source": [
        "for alpha in alphas:\n",
        "  print('Alpha: ', alpha)\n",
        "  print('Score: ', train_and_predict(alpha, sem_tfidf_train, sem_tfidf_test))\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bslIOnaPcrfl"
      },
      "source": [
        "### Naive-Bayes (with best parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl5gYcT6cwks"
      },
      "source": [
        "nb_classifier = MultinomialNB(alpha=0.1)\n",
        "nb_classifier.fit(sem_tfidf_train, y_train)\n",
        "\n",
        "pred = nb_classifier.predict(sem_tfidf_test)\n",
        "print(classification_report(y_test, pred))\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['FALSE', 'TRUE', 'PARTIALLY FALSE', 'OTHER'])\n",
        "print('Confusion Matrix: ')\n",
        "print(cm)\n",
        "plot_confusion_matrix(cm, classes=['FALSE', 'TRUE', 'PARTIALLY', 'OTHER'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1Xi6CySd8in"
      },
      "source": [
        "### K Nearest Neighbors Hyper-Parameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOuV4AC8eALE"
      },
      "source": [
        "knn_params = RandomizedSearchCV(estimator=KNeighborsClassifier(), param_distributions={\n",
        "    \"leaf_size\": list(range(1, 50)),\n",
        "    \"n_neighbors\": list(range(1, 30)),\n",
        "    \"p\": [1, 2]\n",
        "}, n_iter=100, n_jobs=-1)\n",
        "\n",
        "result = knn_params.fit(sem_tfidf_train, y_train)\n",
        "result_df = pd.DataFrame(result.cv_results_).loc[[result.best_index_]]\n",
        "\n",
        "print(result_df[\"params\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za5ec5ubeLGc"
      },
      "source": [
        "### K Nearest Neighbors (with best parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5EzywB6eOJ7"
      },
      "source": [
        "knn_classifier = KNeighborsClassifier(p = 2, n_neighbors = 27, leaf_size = 12)\n",
        "knn_classifier.fit(sem_tfidf_train, y_train)\n",
        "\n",
        "pred = knn_classifier.predict(sem_tfidf_test)\n",
        "print(classification_report(y_test, pred))\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['FALSE', 'TRUE', 'PARTIALLY FALSE', 'OTHER'])\n",
        "plot_confusion_matrix(cm, classes=['FALSE', 'TRUE', 'PARTIALLY', 'OTHER'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5EOlaVZhT_4"
      },
      "source": [
        "### Random Forest Hyper-Parameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbB_bkZMhaFI"
      },
      "source": [
        "rf_params = RandomizedSearchCV(estimator=RandomForestClassifier(), param_distributions={\n",
        "    \"n_estimators\" : [200, 400, 600, 800, 1000],\n",
        "    \"max_features\" : ['auto', 'sqrt'],\n",
        "    \"max_depth\" : [10, 20, 30, 40, 50, None],\n",
        "    \"min_samples_split\" : [2, 5, 10],\n",
        "    \"min_samples_leaf\" : [1, 2, 4]\n",
        "}, n_iter=10, n_jobs=-1)\n",
        "\n",
        "result = rf_params.fit(sem_tfidf_train, y_train)\n",
        "result_df = pd.DataFrame(result.cv_results_).loc[[result.best_index_]]\n",
        "\n",
        "print(result_df[\"params\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXWqnFdThzrd"
      },
      "source": [
        "### Random Forest (with best parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYLRAz3kh2bF"
      },
      "source": [
        "rf_classifier = RandomForestClassifier(n_estimators = 200, min_samples_split = 10, min_samples_leaf = 1, max_features = 'sqrt', max_depth = 30)\n",
        "rf_classifier.fit(sem_tfidf_train, y_train)\n",
        "\n",
        "pred = rf_classifier.predict(sem_tfidf_test)\n",
        "print(classification_report(y_test, pred))\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['FALSE', 'TRUE', 'PARTIALLY FALSE', 'OTHER'])\n",
        "print('Confusion Matrix: ')\n",
        "print(cm)\n",
        "plot_confusion_matrix(cm, classes=['FALSE', 'TRUE', 'PARTIALLY', 'OTHER'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIadm7Ehip4_"
      },
      "source": [
        "### Gradient Boosting (with default parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciuX56ZoitYD"
      },
      "source": [
        "gb_classifier = GradientBoostingClassifier(verbose=True, n_estimators = 200)\n",
        "gb_classifier.fit(sem_tfidf_train, y_train)\n",
        "\n",
        "pred = gb_classifier.predict(sem_tfidf_test)\n",
        "print(classification_report(y_test, pred))\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['FALSE', 'TRUE', 'PARTIALLY FALSE', 'OTHER'])\n",
        "print('Confusion Matrix: ')\n",
        "print(cm)\n",
        "plot_confusion_matrix(cm, classes=['FALSE', 'TRUE', 'PARTIALLY', 'OTHER'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z3OFuIsjT1x"
      },
      "source": [
        "# Fourth Approach - Three-Layered Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Rzwq6omjaHX"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw2y5KGZjhzB"
      },
      "source": [
        "X = data.drop('our rating', axis = 1)\n",
        "y = data['our rating']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0AAD9yznLW_"
      },
      "source": [
        "## Train and Test Splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "059epSaMkaif"
      },
      "source": [
        "X_train_text = X_train['clean_text']\n",
        "X_test_text = X_test['clean_text']\n",
        "\n",
        "X_train_POS = X_train['POS_text']\n",
        "X_test_POS = X_test['POS_text']\n",
        "\n",
        "X_train_sem = X_train['semantics_text']\n",
        "X_test_sem = X_test['semantics_text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrBpBL_InTI4"
      },
      "source": [
        "### TFIDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRQr8loDnPXN"
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range = (1,3))\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(X_train_text.astype('str'))\n",
        "tfidf_test = tfidf_vectorizer.transform(X_test_text.astype('str'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRKfnF-GrgO4"
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range = (1,3))\n",
        "tfidf_vectorizer.fit(X_train_text.astype('str'))\n",
        "\n",
        "from sklearn import model_selection\n",
        "import pickle\n",
        "\n",
        "pickled_tfidf = 'tfidf_pickle.sav'\n",
        "pickle.dump(tfidf_vectorizer, open(pickled_tfidf, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpFJVyeOnYUC"
      },
      "source": [
        "### POS Tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz-jCMPznemf"
      },
      "source": [
        "pos_tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range = (1,3))\n",
        "pos_tfidf_train = pos_tfidf_vectorizer.fit_transform(X_train_POS.astype('str'))\n",
        "\n",
        "pos_tfidf_test = pos_tfidf_vectorizer.transform(X_test_POS.astype('str'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jnxf0VwN306P"
      },
      "source": [
        "pos_tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range = (1,3))\n",
        "pos_tfidf_vectorizer.fit(X_train_POS.astype('str'))\n",
        "\n",
        "pickled_pos = 'pos_pickle.sav'\n",
        "pickle.dump(pos_tfidf_vectorizer, open(pickled_pos, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq6EhqE9nf3V"
      },
      "source": [
        "### Semantic Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nQCXpZ7njCK"
      },
      "source": [
        "sem_tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range = (1,1))\n",
        "sem_tfidf_train = sem_tfidf_vectorizer.fit_transform(X_train_sem.astype('str'))\n",
        "\n",
        "sem_tfidf_test = sem_tfidf_vectorizer.transform(X_test_sem.astype('str'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Klk1KFQo4GFT"
      },
      "source": [
        "sem_tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range = (1,1))\n",
        "sem_tfidf_vectorizer.fit(X_train_sem.astype('str'))\n",
        "\n",
        "pickled_sem = 'sem_pickle.sav'\n",
        "pickle.dump(sem_tfidf_vectorizer, open(pickled_sem, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Evn_9GZcoFY1"
      },
      "source": [
        "### Weight Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxoGZ64GoIoG"
      },
      "source": [
        "text_w = 0.5 * 3\n",
        "pos_w = 0.15 * 3\n",
        "sem_w = 0.35 * 3\n",
        "\n",
        "tfidf_train *= text_w\n",
        "tfidf_test *= text_w\n",
        "pos_tfidf_train *= pos_w\n",
        "pos_tfidf_test *= pos_w\n",
        "sem_tfidf_train *= sem_w\n",
        "sem_tfidf_test *= sem_w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFzVjgIBoL1o"
      },
      "source": [
        "diff_n_rows = pos_tfidf_train.shape[0] - tfidf_train.shape[0]\n",
        "b = sp.vstack((tfidf_train, sp.csr_matrix((diff_n_rows, tfidf_train.shape[1]))))\n",
        "c = sp.hstack((pos_tfidf_train, b))\n",
        "\n",
        "diff_n_rows = c.shape[0] - sem_tfidf_train.shape[0]\n",
        "b = sp.vstack((sem_tfidf_train, sp.csr_matrix((diff_n_rows, sem_tfidf_train.shape[1]))))\n",
        "\n",
        "X_train = sp.hstack((c, b))\n",
        "\n",
        "diff_n_rows = pos_tfidf_test.shape[0] - tfidf_test.shape[0]\n",
        "d = sp.vstack((tfidf_test, sp.csr_matrix((diff_n_rows, tfidf_test.shape[1]))))\n",
        "e = sp.hstack((pos_tfidf_test, d))\n",
        "\n",
        "diff_n_rows = e.shape[0] - sem_tfidf_test.shape[0]\n",
        "d = sp.vstack((sem_tfidf_test, sp.csr_matrix((diff_n_rows, sem_tfidf_test.shape[1]))))\n",
        "\n",
        "X_test = sp.hstack((e, d))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syftFPDroROV"
      },
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh5ZUB72oamj"
      },
      "source": [
        "### Naive-Bayes Alpha-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEJE6x6hodv1"
      },
      "source": [
        "for alpha in alphas:\n",
        "  print('Alpha: ', alpha)\n",
        "  print('Score: ', train_and_predict(alpha, X_train, X_test))\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrzZL-vPopjb"
      },
      "source": [
        "### Naive-Bayes (with best parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2E3i6iohovXl"
      },
      "source": [
        "nb_classifier = MultinomialNB(alpha = 0.0)\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "pred = nb_classifier.predict(X_test)\n",
        "print(classification_report(y_test, pred))\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['FALSE', 'TRUE', 'PARTIALLY FALSE', 'OTHER'])\n",
        "print('Confusion Matrix: ')\n",
        "print(cm)\n",
        "plot_confusion_matrix(cm, classes=['FALSE', 'TRUE', 'PARTIALLY', 'OTHER'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBmnxVpRrdhU"
      },
      "source": [
        "### K Nearest Neighbors Hyper-Parameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yXasBFAtop6"
      },
      "source": [
        "knn_params = RandomizedSearchCV(estimator=KNeighborsClassifier(), param_distributions={\n",
        "    \"leaf_size\": list(range(1, 50)),\n",
        "    \"n_neighbors\": list(range(1, 30)),\n",
        "    \"p\": [1, 2]\n",
        "}, n_iter=100, n_jobs=-1)\n",
        "\n",
        "result = knn_params.fit(X_train, y_train)\n",
        "result_df = pd.DataFrame(result.cv_results_).loc[[result.best_index_]]\n",
        "\n",
        "print(result_df[\"params\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LolJKSuSt889"
      },
      "source": [
        "### K Nearest Neighbors (with best parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkaRGMFfuB6s"
      },
      "source": [
        "knn_classifier = KNeighborsClassifier(p = 2, n_neighbors = 19, leaf_size = 6)\n",
        "knn_classifier.fit(X_train, y_train)\n",
        "\n",
        "pred = knn_classifier.predict(X_test)\n",
        "print(classification_report(y_test, pred))\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['FALSE', 'TRUE', 'PARTIALLY FALSE', 'OTHER'])\n",
        "plot_confusion_matrix(cm, classes=['FALSE', 'TRUE', 'PARTIALLY', 'OTHER'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d5ReTuxvK8w"
      },
      "source": [
        "### Random Forest Hyper-Parameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9206YEyvQjc"
      },
      "source": [
        "rf_params = RandomizedSearchCV(estimator=RandomForestClassifier(), param_distributions={\n",
        "    \"n_estimators\" : [200, 400, 600, 800, 1000],\n",
        "    \"max_features\" : ['auto', 'sqrt'],\n",
        "    \"max_depth\" : [10, 20, 30, 40, 50, None],\n",
        "    \"min_samples_split\" : [2, 5, 10],\n",
        "    \"min_samples_leaf\" : [1, 2, 4]\n",
        "}, n_iter=10, n_jobs=-1)\n",
        "\n",
        "result = rf_params.fit(X_train, y_train)\n",
        "result_df = pd.DataFrame(result.cv_results_).loc[[result.best_index_]]\n",
        "\n",
        "print(result_df[\"params\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rt306DRvbGZ"
      },
      "source": [
        "### Random Forest (with best parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GY26M1xtvese"
      },
      "source": [
        "rf_classifier = RandomForestClassifier(n_estimators = 1000, min_samples_split = 10, min_samples_leaf = 2, max_features = 'auto', max_depth = 30)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "pred = rf_classifier.predict(X_test)\n",
        "print(classification_report(y_test, pred))\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['FALSE', 'TRUE', 'PARTIALLY FALSE', 'OTHER'])\n",
        "print('Confusion Matrix: ')\n",
        "print(cm)\n",
        "plot_confusion_matrix(cm, classes=['FALSE', 'TRUE', 'PARTIALLY', 'OTHER'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPWqXvj5wQCx"
      },
      "source": [
        "### Gradient Boosting (with default parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLGok-yBwT_q"
      },
      "source": [
        "gb_classifier = GradientBoostingClassifier(verbose=True, n_estimators = 200)\n",
        "gb_classifier.fit(X_train, y_train)\n",
        "\n",
        "pred = gb_classifier.predict(X_test)\n",
        "print(classification_report(y_test, pred))\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['FALSE', 'TRUE', 'PARTIALLY FALSE', 'OTHER'])\n",
        "print('Confusion Matrix: ')\n",
        "print(cm)\n",
        "plot_confusion_matrix(cm, classes=['FALSE', 'TRUE', 'PARTIALLY', 'OTHER'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy2V4uLuf-H-"
      },
      "source": [
        "gb_classifier = GradientBoostingClassifier(verbose=True, n_estimators = 200)\n",
        "gb_classifier.fit(X_train, y_train)\n",
        "\n",
        "pickled_model = 'three_layer_pickle.sav'\n",
        "pickle.dump(gb_classifier, open(pickled_model, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcFmBEn_hdHD"
      },
      "source": [
        "loaded_model = pickle.load(open(pickled_model, 'rb'))\n",
        "pred = loaded_model.predict(X_test)\n",
        "print(classification_report(y_test, pred))\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['FALSE', 'TRUE', 'PARTIALLY FALSE', 'OTHER'])\n",
        "print('Confusion Matrix: ')\n",
        "print(cm)\n",
        "plot_confusion_matrix(cm, classes=['FALSE', 'TRUE', 'PARTIALLY', 'OTHER'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}